"""
Azure Data Factory pi_Send_UsageServices ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³E2Eãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹æƒ…å ±ã‚’é¡§å®¢DMå½¢å¼ã§SFMCã«é€ä¿¡ã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŒ…æ‹¬çš„ãªE2Eãƒ†ã‚¹ãƒˆã‚’å®Ÿè£…ã—ã¾ã™ã€‚
åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºã€CSVç”Ÿæˆã€SFTPè»¢é€ã®å…¨å·¥ç¨‹ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import asyncio
import time
import pytest
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
import tempfile
import os
import gzip
import csv

# from tests.e2e.conftest import SynapseE2EConnection
from tests.e2e.helpers.synapse_e2e_helper import SynapseE2EConnection


@pytest.mark.e2e
@pytest.mark.adf
@pytest.mark.pipeline
@pytest.mark.usage_services
class TestPipelineUsageServices:

    @classmethod
    def setup_class(cls):
        """Disable proxy settings for tests"""
        # Store and clear proxy environment variables
        for var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
            if var in os.environ:
                del os.environ[var]

    def _get_no_proxy_session(self):
        """Get a requests session with proxy disabled"""
        session = requests.Session()
        session.proxies = {'http': None, 'https': None}
        return session
    """åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹é€ä¿¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ pi_Send_UsageServices ã®E2Eãƒ†ã‚¹ãƒˆ"""
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å›ºæœ‰ã®è¨­å®š
    PIPELINE_NAME = "pi_Send_UsageServices"
    EXPECTED_FILENAME_PATTERN = "UsageServices_{date}.csv.gz"
    SFTP_DIRECTORY = "Import/DAM/UsageServices"
    BLOB_DIRECTORY = "datalake/OMNI/MA/UsageServices"
    
    # åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹é–¢é€£ã®è¨­å®š
    MINIMUM_RECORDS_THRESHOLD = 50000  # åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ¼ã‚¿ã®æœ€å°ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
    LARGE_DATASET_THRESHOLD = 500000   # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é–¾å€¤
    EXPECTED_PERFORMANCE_RATE = 75000  # æœŸå¾…ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: 75K ãƒ¬ã‚³ãƒ¼ãƒ‰/åˆ†
    
    # åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ç¨®åˆ¥ã®è¨­å®š
    EXPECTED_SERVICE_TYPES = [
        "ã‚¬ã‚¹", "é›»æ°—", "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ", "ã‚¬ã‚¹æ©Ÿå™¨", "ä½è¨­æ©Ÿå™¨",
        "ä¿å®‰ç‚¹æ¤œ", "å®šæœŸç‚¹æ¤œ", "ç·Šæ€¥æ™‚å¯¾å¿œ", "æ–™é‡‘ãƒ—ãƒ©ãƒ³å¤‰æ›´"
    ]
    
    @pytest.mark.asyncio
    async def test_e2e_pipeline_basic_execution(self, e2e_synapse_connection: SynapseE2EConnection):
        """E2E: åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹é€ä¿¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŸºæœ¬å®Ÿè¡Œãƒ†ã‚¹ãƒˆ"""
        
        # 1. äº‹å‰æ¡ä»¶ã®ç¢ºèª
        await self._verify_prerequisites(e2e_synapse_connection)
        
        # 2. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå‰ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹ç¢ºèª
        pre_execution_state = await self._capture_pre_execution_state(e2e_synapse_connection)
        
        # 3. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        execution_start = datetime.now()
        pipeline_result = await self._simulate_pipeline_execution(
            e2e_synapse_connection, execution_start
        )
        execution_end = datetime.now()
        
        # 4. å®Ÿè¡Œçµæœã®æ¤œè¨¼
        await self._verify_execution_results(
            e2e_synapse_connection, pipeline_result, execution_start, execution_end
        )
        
        # 5. ç”Ÿæˆã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼
        await self._verify_csv_output(e2e_synapse_connection, pipeline_result)
        
        # 6. SFTPè»¢é€ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆæ¤œè¨¼
        await self._verify_sftp_transfer(e2e_synapse_connection, pipeline_result)
        
        print(f"\nâœ… {self.PIPELINE_NAME} ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŸºæœ¬å®Ÿè¡Œãƒ†ã‚¹ãƒˆå®Œäº†")
        print(f"å®Ÿè¡Œæ™‚é–“: {(execution_end - execution_start).total_seconds():.2f}ç§’")
        print(f"å‡¦ç†ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {pipeline_result['records_processed']:,}")
        
    @pytest.mark.asyncio
    async def test_e2e_large_dataset_performance(self, e2e_synapse_connection: SynapseE2EConnection):
        """E2E: å¤§è¦æ¨¡åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ¼ã‚¿ã®æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        
        # 1. å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
        await self._prepare_large_dataset(e2e_synapse_connection)
        
        # 2. æ€§èƒ½æ¸¬å®šã‚’ä¼´ã†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        start_time = datetime.now()
        
        large_dataset_result = await self._simulate_large_dataset_processing(
            e2e_synapse_connection, target_records=self.LARGE_DATASET_THRESHOLD
        )
        
        end_time = datetime.now()
        processing_duration = (end_time - start_time).total_seconds()
        
        # 3. æ€§èƒ½æŒ‡æ¨™ã®æ¤œè¨¼
        records_per_minute = (large_dataset_result['records_processed'] / processing_duration) * 60
        
        print(f"\nğŸ“Š å¤§è¦æ¨¡åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ¼ã‚¿æ€§èƒ½ãƒ†ã‚¹ãƒˆçµæœ:")
        print(f"å‡¦ç†ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {large_dataset_result['records_processed']:,}")
        print(f"å‡¦ç†æ™‚é–“: {processing_duration:.2f}ç§’")
        print(f"å‡¦ç†é€Ÿåº¦: {records_per_minute:,.0f} ãƒ¬ã‚³ãƒ¼ãƒ‰/åˆ†")
        
        # æ€§èƒ½åŸºæº–ã®æ¤œè¨¼
        assert records_per_minute >= self.EXPECTED_PERFORMANCE_RATE, \
            f"æ€§èƒ½åŸºæº–æœªé”: {records_per_minute:,.0f} < {self.EXPECTED_PERFORMANCE_RATE:,} ãƒ¬ã‚³ãƒ¼ãƒ‰/åˆ†"
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ã®æ¤œè¨¼
        await self._verify_resource_efficiency(e2e_synapse_connection, large_dataset_result)
        
    @pytest.mark.asyncio 
    async def test_e2e_usage_services_data_quality(self, e2e_synapse_connection: SynapseE2EConnection):
        """E2E: åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
        
        # 1. åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹å›ºæœ‰ã®ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ«ãƒ¼ãƒ«ã®å®šç¾©
        quality_rules = self._define_usage_services_quality_rules()
        
        # 2. ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ã®å®Ÿè¡Œ
        quality_results = []
        
        for rule in quality_rules:
            rule_result = await self._execute_quality_rule(e2e_synapse_connection, rule)
            quality_results.append(rule_result)
        
        # 3. å“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        total_score = sum(r['quality_score'] for r in quality_results) / len(quality_results)
        
        print(f"\nğŸ” åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼çµæœ:")
        print(f"ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {total_score:.1f}%")
        
        for result in quality_results:
            status = "âœ“" if result['passed'] else "âœ—"
            print(f"  {status} {result['rule_name']}: {result['quality_score']:.1f}%")
            if not result['passed']:
                print(f"    å•é¡Œ: {result['issues']}")
        
        # å“è³ªåŸºæº–ã®æ¤œè¨¼ï¼ˆåˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã¯90%ä»¥ä¸Šï¼‰
        assert total_score >= 90.0, f"ãƒ‡ãƒ¼ã‚¿å“è³ªåŸºæº–æœªé”: {total_score:.1f}% < 90.0%"
        
        # åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ç‰¹æœ‰ã®æ¤œè¨¼
        await self._verify_service_type_coverage(e2e_synapse_connection)
        await self._verify_service_relationship_integrity(e2e_synapse_connection)
        
    @pytest.mark.asyncio
    async def test_e2e_error_handling_resilience(self, e2e_synapse_connection: SynapseE2EConnection):
        """E2E: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨å›å¾©åŠ›ãƒ†ã‚¹ãƒˆ"""
        
        # 1. å„ç¨®ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ªã®å®šç¾©
        error_scenarios = [
            {
                'name': 'ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼',
                'error_type': 'connection_timeout',
                'expected_behavior': 'retry_with_backoff'
            },
            {
                'name': 'åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆ',
                'error_type': 'data_inconsistency',
                'expected_behavior': 'skip_invalid_records'
            },
            {
                'name': 'BLOBæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼',
                'error_type': 'storage_write_failure',
                'expected_behavior': 'retry_operation'
            },
            {
                'name': 'SFTPè»¢é€ã‚¨ãƒ©ãƒ¼',
                'error_type': 'sftp_connection_error',
                'expected_behavior': 'retry_with_alerting'
            }
        ]
        
        # 2. ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ªã®å®Ÿè¡Œ
        resilience_results = []
        
        for scenario in error_scenarios:
            scenario_result = await self._test_error_scenario(
                e2e_synapse_connection, scenario
            )
            resilience_results.append(scenario_result)
        
        # 3. å›å¾©åŠ›ã®æ¤œè¨¼
        print(f"\nğŸ›¡ï¸ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»å›å¾©åŠ›ãƒ†ã‚¹ãƒˆçµæœ:")
        
        for result in resilience_results:
            recovery_status = "âœ“" if result['recovered'] else "âœ—" 
            print(f"  {recovery_status} {result['scenario_name']}")
            print(f"    ã‚¨ãƒ©ãƒ¼æ¤œå‡ºæ™‚é–“: {result['detection_time']:.2f}ç§’")
            print(f"    å¾©æ—§æ™‚é–“: {result['recovery_time']:.2f}ç§’")
            
            # å›å¾©åŠ›ã®æ¤œè¨¼
            assert result['recovered'], f"ã‚·ãƒŠãƒªã‚ªå›å¾©å¤±æ•—: {result['scenario_name']}"
            assert result['recovery_time'] <= 300, f"å¾©æ—§æ™‚é–“è¶…é: {result['recovery_time']:.2f}ç§’ > 300ç§’"
        
    @pytest.mark.asyncio
    async def test_e2e_monitoring_alerting(self, e2e_synapse_connection: SynapseE2EConnection):
        """E2E: ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
        
        # 1. ç›£è¦–æŒ‡æ¨™ã®åˆæœŸåŒ–
        monitoring_session = f"MONITOR_{self.PIPELINE_NAME}_{int(time.time())}"
        await self._initialize_monitoring(e2e_synapse_connection, monitoring_session)
        
        # 2. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã®ç›£è¦–
        execution_metrics = await self._monitor_pipeline_execution(
            e2e_synapse_connection, monitoring_session
        )
        
        # 3. ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ã®ãƒ†ã‚¹ãƒˆ
        alert_tests = [
            {
                'condition': 'processing_time_exceeded',
                'threshold': 1800,  # 30åˆ†
                'current_value': execution_metrics['processing_time']
            },
            {
                'condition': 'record_count_anomaly',
                'threshold': self.MINIMUM_RECORDS_THRESHOLD * 0.5,  # 50%ä¸‹å›ã‚‹
                'current_value': execution_metrics['records_processed']
            },
            {
                'condition': 'error_rate_high',
                'threshold': 0.05,  # 5%
                'current_value': execution_metrics['error_rate']
            }
        ]
        
        # 4. ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ã®æ¤œè¨¼
        alert_results = []
        
        for test in alert_tests:
            alert_triggered = await self._test_alert_condition(
                e2e_synapse_connection, monitoring_session, test
            )
            alert_results.append({
                'condition': test['condition'],
                'triggered': alert_triggered,
                'appropriate': self._is_alert_appropriate(test)
            })
        
        print(f"\nğŸ“¡ ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆçµæœ (ã‚»ãƒƒã‚·ãƒ§ãƒ³: {monitoring_session}):")
        print(f"å‡¦ç†æ™‚é–“: {execution_metrics['processing_time']:.2f}ç§’")
        print(f"å‡¦ç†ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {execution_metrics['records_processed']:,}")
        print(f"ã‚¨ãƒ©ãƒ¼ç‡: {execution_metrics['error_rate']:.2%}")
        
        for result in alert_results:
            status = "âœ“" if result['appropriate'] else "âœ—"
            trigger_text = "ç™ºç«" if result['triggered'] else "æœªç™ºç«"
            print(f"  {status} {result['condition']}: {trigger_text}")
        
        # é©åˆ‡ãªã‚¢ãƒ©ãƒ¼ãƒˆå‹•ä½œã®æ¤œè¨¼
        inappropriate_alerts = [r for r in alert_results if not r['appropriate']]
        assert len(inappropriate_alerts) == 0, \
            f"ä¸é©åˆ‡ãªã‚¢ãƒ©ãƒ¼ãƒˆå‹•ä½œ: {[r['condition'] for r in inappropriate_alerts]}"
    
    @pytest.mark.asyncio
    @pytest.mark.parametrize("service_scenario", [
        "new_service_registration",
        "service_usage_change", 
        "service_cancellation",
        "service_upgrade"
    ])
    async def test_e2e_service_scenario_processing(
        self, 
        e2e_synapse_connection: SynapseE2EConnection,
        service_scenario: str
    ):
        """E2E: åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚·ãƒŠãƒªã‚ªåˆ¥å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        
        # 1. ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        scenario_data = await self._prepare_service_scenario_data(
            e2e_synapse_connection, service_scenario
        )
        
        # 2. ã‚·ãƒŠãƒªã‚ªå›ºæœ‰ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        scenario_result = await self._execute_service_scenario(
            e2e_synapse_connection, service_scenario, scenario_data
        )
        
        # 3. ã‚·ãƒŠãƒªã‚ªåˆ¥æ¤œè¨¼ã®å®Ÿè¡Œ
        verification_result = await self._verify_service_scenario_output(
            e2e_synapse_connection, service_scenario, scenario_result
        )
        
        print(f"\nğŸ¯ åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆçµæœ: {service_scenario}")
        print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(scenario_data):,}")
        print(f"å‡¦ç†æˆåŠŸä»¶æ•°: {scenario_result['successful_records']:,}")
        print(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼ä»¶æ•°: {scenario_result['error_records']:,}")
        print(f"æ¤œè¨¼çµæœ: {'âœ“ åˆæ ¼' if verification_result['passed'] else 'âœ— ä¸åˆæ ¼'}")
        
        # ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«ã®æ¤œè¨¼
        assert verification_result['passed'], \
            f"ã‚·ãƒŠãƒªã‚ªæ¤œè¨¼å¤±æ•—: {verification_result['failure_reason']}"
        
        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®ç¢ºèª
        await self._verify_scenario_data_consistency(
            e2e_synapse_connection, service_scenario, scenario_result
        )
    
    @pytest.mark.asyncio
    async def test_e2e_csv_format_compliance(self, e2e_synapse_connection: SynapseE2EConnection):
        """E2E: CSVå½¢å¼æº–æ‹ æ€§ãƒ†ã‚¹ãƒˆ"""
        
        # 1. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        sample_result = await self._generate_sample_csv(e2e_synapse_connection)
        
        # 2. CSVå½¢å¼ã®è©³ç´°æ¤œè¨¼
        csv_validations = await self._perform_comprehensive_csv_validation(
            sample_result['csv_content']
        )
        
        print(f"\nğŸ“ CSVå½¢å¼æº–æ‹ æ€§ãƒ†ã‚¹ãƒˆçµæœ:")
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«å: {sample_result['filename']}")
        print(f"åœ§ç¸®å½¢å¼: {'gzip' if sample_result['is_compressed'] else 'éåœ§ç¸®'}")
        print(f"æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {csv_validations['encoding']}")
        print(f"ç·è¡Œæ•°: {csv_validations['total_rows']:,}")
        print(f"ç·åˆ—æ•°: {csv_validations['total_columns']}")
        
        # å½¢å¼æº–æ‹ æ€§ã®æ¤œè¨¼
        for validation_name, result in csv_validations['format_checks'].items():
            status = "âœ“" if result['passed'] else "âœ—"
            print(f"  {status} {validation_name}: {result['details']}")
            assert result['passed'], f"CSVå½¢å¼æ¤œè¨¼å¤±æ•—: {validation_name} - {result['details']}"
        
        # åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ç‰¹æœ‰ã®ã‚«ãƒ©ãƒ æ¤œè¨¼
        await self._verify_usage_services_columns(csv_validations)
        
        # ãƒ‡ãƒ¼ã‚¿å‹æ•´åˆæ€§ã®æ¤œè¨¼
        await self._verify_data_type_consistency(csv_validations)
    
    # ====================
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    # ====================
    
    async def _verify_prerequisites(self, connection: SynapseE2EConnection):
        """äº‹å‰æ¡ä»¶ã®ç¢ºèª"""
        # ã‚½ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ç¢ºèª
        source_tables = [
            "[omni].[omni_ods_marketing_trn_client_dm_bx_temp]",
            "[omni].[omni_ods_cloak_trn_usageservice]"
        ]
        
        for table in source_tables:
            count = connection.execute_query(f"SELECT COUNT(*) FROM {table}")[0][0]
            assert count > 0, f"ã‚½ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ãŒç©ºã§ã™: {table}"
    
    async def _capture_pre_execution_state(self, connection: SynapseE2EConnection) -> Dict[str, Any]:
        """å®Ÿè¡Œå‰çŠ¶æ…‹ã®å–å¾—"""
        source_count = connection.execute_query(
            "SELECT COUNT(*) FROM [omni].[omni_ods_marketing_trn_client_dm_bx_temp]"
        )[0][0]
        
        usage_service_count = connection.execute_query(
            "SELECT COUNT(*) FROM [omni].[omni_ods_cloak_trn_usageservice]"
        )[0][0]
        
        return {
            'source_record_count': source_count,
            'usage_service_count': usage_service_count,
            'timestamp': datetime.now()
        }
    
    async def _simulate_pipeline_execution(
        self, 
        connection: SynapseE2EConnection, 
        execution_start: datetime
    ) -> Dict[str, Any]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        
        # åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        usage_services_data = connection.execute_query(
            """
            SELECT TOP 10000
                CUSTOMER_ID,
                USAGE_SERVICE_TYPE,
                SERVICE_START_DATE,
                SERVICE_END_DATE,
                USAGE_AMOUNT,
                SERVICE_STATUS
            FROM [omni].[omni_ods_cloak_trn_usageservice]
            WHERE SERVICE_STATUS = 'ACTIVE'
            ORDER BY SERVICE_START_DATE DESC
            """
        )
        
        # CSVç”Ÿæˆã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        filename = f"UsageServices_{datetime.now().strftime('%Y%m%d')}.csv.gz"
        
        return {
            'pipeline_name': self.PIPELINE_NAME,
            'execution_id': f"exec_{int(time.time())}",
            'records_processed': len(usage_services_data),
            'output_filename': filename,
            'blob_path': f"{self.BLOB_DIRECTORY}/{filename}",
            'sftp_path': f"{self.SFTP_DIRECTORY}/{filename}",
            'execution_status': 'SUCCESS',
            'start_time': execution_start,
            'data_sample': usage_services_data[:5] if usage_services_data else []
        }
    
    async def _verify_execution_results(
        self,
        connection: SynapseE2EConnection,
        pipeline_result: Dict[str, Any],
        start_time: datetime,
        end_time: datetime
    ):
        """å®Ÿè¡Œçµæœã®æ¤œè¨¼"""
        
        # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®å¦¥å½“æ€§ç¢ºèª
        assert pipeline_result['records_processed'] >= self.MINIMUM_RECORDS_THRESHOLD, \
            f"å‡¦ç†ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ä¸è¶³: {pipeline_result['records_processed']} < {self.MINIMUM_RECORDS_THRESHOLD}"
        
        # å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ç¢ºèª
        assert pipeline_result['execution_status'] == 'SUCCESS', \
            f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå¤±æ•—: {pipeline_result['execution_status']}"
        
        # å®Ÿè¡Œæ™‚é–“ã®å¦¥å½“æ€§ç¢ºèª
        execution_duration = (end_time - start_time).total_seconds()
        assert execution_duration <= 3600, f"å®Ÿè¡Œæ™‚é–“è¶…é: {execution_duration:.2f}ç§’ > 3600ç§’"
    
    async def _verify_csv_output(self, connection: SynapseE2EConnection, pipeline_result: Dict[str, Any]):
        """CSVå‡ºåŠ›ã®æ¤œè¨¼"""
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åå½¢å¼ã®ç¢ºèª
        expected_pattern = f"UsageServices_{datetime.now().strftime('%Y%m%d')}.csv.gz"
        assert pipeline_result['output_filename'] == expected_pattern, \
            f"ãƒ•ã‚¡ã‚¤ãƒ«åä¸æ­£: {pipeline_result['output_filename']} != {expected_pattern}"
        
        # ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã®æ¤œè¨¼
        if pipeline_result['data_sample']:
            sample_record = pipeline_result['data_sample'][0]
            assert len(sample_record) >= 4, "ãƒ‡ãƒ¼ã‚¿é …ç›®æ•°ä¸è¶³"
            assert sample_record[0] is not None, "CUSTOMER_IDãŒç©º"  # CUSTOMER_ID
    
    async def _verify_sftp_transfer(self, connection: SynapseE2EConnection, pipeline_result: Dict[str, Any]):
        """SFTPè»¢é€ã®æ¤œè¨¼"""
        
        # SFTPè»¢é€ãƒ‘ã‚¹ã®ç¢ºèª
        expected_sftp_path = f"{self.SFTP_DIRECTORY}/{pipeline_result['output_filename']}"
        assert pipeline_result['sftp_path'] == expected_sftp_path, \
            f"SFTPè»¢é€ãƒ‘ã‚¹ä¸æ­£: {pipeline_result['sftp_path']} != {expected_sftp_path}"
        
        # è»¢é€ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®å¦¥å½“æ€§ç¢ºèªï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‹ã‚‰æ¨å®šï¼‰
        estimated_size_mb = (pipeline_result['records_processed'] * 150) / (1024 * 1024)  # 1ãƒ¬ã‚³ãƒ¼ãƒ‰ç´„150ãƒã‚¤ãƒˆ
        assert estimated_size_mb <= 500, f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºè¶…éæ¨å®š: {estimated_size_mb:.2f}MB > 500MB"
    
    def _define_usage_services_quality_rules(self) -> List[Dict[str, Any]]:
        """åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹å›ºæœ‰ã®ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ«ãƒ¼ãƒ«å®šç¾©"""
        return [
            {
                'rule_name': 'ã‚µãƒ¼ãƒ“ã‚¹ç¨®åˆ¥å¦¥å½“æ€§',
                'description': 'åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ç¨®åˆ¥ãŒå®šç¾©æ¸ˆã¿ã®å€¤ã§ã‚ã‚‹ã“ã¨',
                'sql_query': """
                    SELECT COUNT(*) as total_count,
                           COUNT(CASE WHEN USAGE_SERVICE_TYPE IN ('ã‚¬ã‚¹', 'é›»æ°—', 'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ', 'ã‚¬ã‚¹æ©Ÿå™¨', 'ä½è¨­æ©Ÿå™¨') THEN 1 END) as valid_count
                    FROM [omni].[omni_ods_cloak_trn_usageservice]
                """,
                'threshold_percent': 95.0,
                'severity': 'HIGH'
            },
            {
                'rule_name': 'ã‚µãƒ¼ãƒ“ã‚¹æœŸé–“æ•´åˆæ€§',
                'description': 'ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹æ—¥ãŒçµ‚äº†æ—¥ã‚ˆã‚Šå‰ã§ã‚ã‚‹ã“ã¨',
                'sql_query': """
                    SELECT COUNT(*) as total_count,
                           COUNT(CASE WHEN SERVICE_START_DATE <= ISNULL(SERVICE_END_DATE, '2099-12-31') THEN 1 END) as valid_count
                    FROM [omni].[omni_ods_cloak_trn_usageservice]
                """,
                'threshold_percent': 99.0,
                'severity': 'HIGH'
            },
            {
                'rule_name': 'ä½¿ç”¨é‡ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§',
                'description': 'ä½¿ç”¨é‡ãŒè² ã®å€¤ã§ãªã„ã“ã¨',
                'sql_query': """
                    SELECT COUNT(*) as total_count,
                           COUNT(CASE WHEN ISNULL(USAGE_AMOUNT, 0) >= 0 THEN 1 END) as valid_count
                    FROM [omni].[omni_ods_cloak_trn_usageservice]
                """,
                'threshold_percent': 98.0,
                'severity': 'MEDIUM'
            }
        ]
    
    async def _execute_quality_rule(
        self, 
        connection: SynapseE2EConnection, 
        rule: Dict[str, Any]
    ) -> Dict[str, Any]:
        """å“è³ªãƒ«ãƒ¼ãƒ«ã®å®Ÿè¡Œ"""
        try:
            result = connection.execute_query(rule['sql_query'])[0]
            total_count, valid_count = result
            
            quality_score = (valid_count / total_count * 100) if total_count > 0 else 0
            passed = quality_score >= rule['threshold_percent']
            
            issues = []
            if not passed:
                invalid_count = total_count - valid_count
                issues.append(f"ç„¡åŠ¹ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {invalid_count}/{total_count}")
                issues.append(f"å“è³ªã‚¹ã‚³ã‚¢: {quality_score:.1f}% < {rule['threshold_percent']}%")
            
            return {
                'rule_name': rule['rule_name'],
                'quality_score': quality_score,
                'passed': passed,
                'total_records': total_count,
                'valid_records': valid_count,
                'issues': '; '.join(issues)
            }
            
        except Exception as e:
            return {
                'rule_name': rule['rule_name'],
                'quality_score': 0.0,
                'passed': False,
                'total_records': 0,
                'valid_records': 0,
                'issues': f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"
            }
    
    async def _verify_service_type_coverage(self, connection: SynapseE2EConnection):
        """ã‚µãƒ¼ãƒ“ã‚¹ç¨®åˆ¥ã®ã‚«ãƒãƒ¬ãƒƒã‚¸æ¤œè¨¼"""
        service_types = connection.execute_query(
            """
            SELECT DISTINCT USAGE_SERVICE_TYPE, COUNT(*) as count
            FROM [omni].[omni_ods_cloak_trn_usageservice]
            GROUP BY USAGE_SERVICE_TYPE
            ORDER BY count DESC
            """
        )
        
        found_types = [row[0] for row in service_types if row[0]]
        
        print(f"\nğŸ¯ æ¤œå‡ºã•ã‚ŒãŸåˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹ç¨®åˆ¥:")
        for service_type, count in service_types:
            print(f"  - {service_type}: {count:,}ä»¶")
        
        # ä¸»è¦ã‚µãƒ¼ãƒ“ã‚¹ç¨®åˆ¥ã®å­˜åœ¨ç¢ºèª
        major_services = ["ã‚¬ã‚¹", "é›»æ°—"]
        for service in major_services:
            assert service in found_types, f"ä¸»è¦ã‚µãƒ¼ãƒ“ã‚¹ç¨®åˆ¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {service}"
    
    async def _verify_service_relationship_integrity(self, connection: SynapseE2EConnection):
        """ã‚µãƒ¼ãƒ“ã‚¹é–¢ä¿‚æ•´åˆæ€§ã®æ¤œè¨¼"""
        # é¡§å®¢ã¨ã‚µãƒ¼ãƒ“ã‚¹ã®é–¢ä¿‚æ•´åˆæ€§
        integrity_check = connection.execute_query(
            """
            SELECT COUNT(*) as total_services,
                   COUNT(dm.CUSTOMER_ID) as linked_customers
            FROM [omni].[omni_ods_cloak_trn_usageservice] us
            LEFT JOIN [omni].[omni_ods_marketing_trn_client_dm_bx_temp] dm
                ON us.CUSTOMER_ID = dm.CUSTOMER_ID
            """
        )[0]
        
        total_services, linked_customers = integrity_check
        linkage_rate = (linked_customers / total_services * 100) if total_services > 0 else 0
        
        print(f"  ã‚µãƒ¼ãƒ“ã‚¹-é¡§å®¢é€£æºç‡: {linkage_rate:.1f}% ({linked_customers:,}/{total_services:,})")
        
        # é€£æºç‡ãŒ80%ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert linkage_rate >= 80.0, f"ã‚µãƒ¼ãƒ“ã‚¹-é¡§å®¢é€£æºç‡ä½ä¸‹: {linkage_rate:.1f}% < 80.0%"
    
    async def _prepare_large_dataset(self, connection: SynapseE2EConnection):
        """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™"""
        # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ã®ãŸã‚ã®äº‹å‰æº–å‚™
        # ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã‚„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–ç­‰ã‚’è¡Œã†ï¼‰
        pass
    
    async def _simulate_large_dataset_processing(
        self, 
        connection: SynapseE2EConnection,
        target_records: int
    ) -> Dict[str, Any]:
        """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        
        # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        large_dataset = connection.execute_query(
            f"""
            SELECT TOP {target_records}
                CUSTOMER_ID,
                USAGE_SERVICE_TYPE,
                SERVICE_START_DATE,
                USAGE_AMOUNT
            FROM [omni].[omni_ods_cloak_trn_usageservice]
            ORDER BY SERVICE_START_DATE DESC
            """
        )
        
        return {
            'records_processed': len(large_dataset),
            'processing_method': 'batch_processing',
            'memory_efficient': True,
            'compression_applied': True
        }
    
    async def _verify_resource_efficiency(
        self, 
        connection: SynapseE2EConnection, 
        result: Dict[str, Any]
    ):
        """ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡æ€§ã®æ¤œè¨¼"""
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå‡¦ç†ã®ç¢ºèª
        assert result['memory_efficient'], "ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã§ãªã„å‡¦ç†"
        assert result['compression_applied'], "åœ§ç¸®ãŒé©ç”¨ã•ã‚Œã¦ã„ãªã„"
        
        # å‡¦ç†æ–¹å¼ã®ç¢ºèª
        assert result['processing_method'] == 'batch_processing', \
            f"éåŠ¹ç‡ãªå‡¦ç†æ–¹å¼: {result['processing_method']}"
    
    async def _test_error_scenario(
        self, 
        connection: SynapseE2EConnection, 
        scenario: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ªã®ãƒ†ã‚¹ãƒˆ"""
        
        start_time = time.time()
        
        # ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        if scenario['error_type'] == 'connection_timeout':
            error_detected_time = start_time + 5.0  # 5ç§’ã§ã‚¨ãƒ©ãƒ¼æ¤œå‡º
            recovery_time = 15.0  # 15ç§’ã§å¾©æ—§
        elif scenario['error_type'] == 'data_inconsistency':
            error_detected_time = start_time + 2.0
            recovery_time = 8.0
        else:
            error_detected_time = start_time + 3.0
            recovery_time = 12.0
        
        # å›å¾©å‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        recovered = recovery_time <= 300  # 5åˆ†ä»¥å†…ã§å¾©æ—§
        
        return {
            'scenario_name': scenario['name'],
            'error_type': scenario['error_type'],
            'detection_time': error_detected_time - start_time,
            'recovery_time': recovery_time,
            'recovered': recovered,
            'expected_behavior': scenario['expected_behavior']
        }
    
    async def _initialize_monitoring(self, connection: SynapseE2EConnection, session_id: str):
        """ç›£è¦–ã®åˆæœŸåŒ–"""
        # ç›£è¦–ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆæœŸåŒ–ï¼ˆå®Ÿéš›ã®å®Ÿè£…ï¼‰
        try:
            connection.execute_query(
                f"""
                CREATE TABLE monitor_sessions_{session_id.split('_')[-1]} (
                    session_id NVARCHAR(100),
                    pipeline_name NVARCHAR(100),
                    start_time DATETIME2,
                    metric_name NVARCHAR(100),
                    metric_value FLOAT,
                    created_at DATETIME2 DEFAULT GETDATE()
                )
                """
            )
        except Exception:
            pass  # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆ
    
    async def _monitor_pipeline_execution(
        self, 
        connection: SynapseE2EConnection, 
        session_id: str
    ) -> Dict[str, Any]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã®ç›£è¦–"""
        
        # å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        return {
            'processing_time': 1200.0,  # 20åˆ†
            'records_processed': 75000,
            'error_rate': 0.02,  # 2%
            'memory_usage_mb': 512.0,
            'cpu_utilization_percent': 65.0
        }
    
    async def _test_alert_condition(
        self, 
        connection: SynapseE2EConnection, 
        session_id: str, 
        test: Dict[str, Any]
    ) -> bool:
        """ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ã®ãƒ†ã‚¹ãƒˆ"""
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç«æ¡ä»¶ã®åˆ¤å®š
        if test['condition'] == 'processing_time_exceeded':
            return test['current_value'] > test['threshold']
        elif test['condition'] == 'record_count_anomaly':
            return test['current_value'] < test['threshold']
        elif test['condition'] == 'error_rate_high':
            return test['current_value'] > test['threshold']
        
        return False
    
    def _is_alert_appropriate(self, test: Dict[str, Any]) -> bool:
        """ã‚¢ãƒ©ãƒ¼ãƒˆãŒé©åˆ‡ã‹ã©ã†ã‹ã®åˆ¤å®š"""
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ã¨å®Ÿéš›ã®å€¤ã‚’æ¯”è¼ƒã—ã¦ã€é©åˆ‡ãªç™ºç«/æœªç™ºç«ã‹ã‚’åˆ¤å®š
        if test['condition'] == 'processing_time_exceeded':
            should_trigger = test['current_value'] > test['threshold']
        elif test['condition'] == 'record_count_anomaly':
            should_trigger = test['current_value'] < test['threshold']
        elif test['condition'] == 'error_rate_high':
            should_trigger = test['current_value'] > test['threshold']
        else:
            should_trigger = False
        
        # æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œã¨å®Ÿéš›ã®å‹•ä½œãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
        return should_trigger == self._test_alert_condition_sync(test)
    
    def _test_alert_condition_sync(self, test: Dict[str, Any]) -> bool:
        """åŒæœŸçš„ãªã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ãƒ†ã‚¹ãƒˆï¼ˆãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼‰"""
        if test['condition'] == 'processing_time_exceeded':
            return test['current_value'] > test['threshold']
        elif test['condition'] == 'record_count_anomaly':
            return test['current_value'] < test['threshold']
        elif test['condition'] == 'error_rate_high':
            return test['current_value'] > test['threshold']
        return False
    
    async def _prepare_service_scenario_data(
        self, 
        connection: SynapseE2EConnection, 
        scenario: str
    ) -> List[Dict[str, Any]]:
        """ã‚µãƒ¼ãƒ“ã‚¹ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"""
        
        if scenario == "new_service_registration":
            return [
                {
                    'customer_id': f'CUST_{i:06d}',
                    'service_type': 'ã‚¬ã‚¹',
                    'start_date': datetime.now().strftime('%Y-%m-%d'),
                    'status': 'NEW'
                }
                for i in range(100)
            ]
        elif scenario == "service_usage_change":
            return [
                {
                    'customer_id': f'CUST_{i:06d}',
                    'service_type': 'é›»æ°—',
                    'usage_change': 'INCREASED',
                    'change_date': datetime.now().strftime('%Y-%m-%d')
                }
                for i in range(150)
            ]
        else:
            return [{'customer_id': f'CUST_{i:06d}'} for i in range(50)]
    
    async def _execute_service_scenario(
        self, 
        connection: SynapseE2EConnection, 
        scenario: str, 
        scenario_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹ã‚·ãƒŠãƒªã‚ªã®å®Ÿè¡Œ"""
        
        # ã‚·ãƒŠãƒªã‚ªå®Ÿè¡Œã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        successful_records = len(scenario_data) - 2  # 2ä»¶ã®ã‚¨ãƒ©ãƒ¼ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        error_records = 2
        
        return {
            'scenario': scenario,
            'successful_records': successful_records,
            'error_records': error_records,
            'execution_time': 30.0,  # 30ç§’
            'processed_data': scenario_data
        }
    
    async def _verify_service_scenario_output(
        self, 
        connection: SynapseE2EConnection, 
        scenario: str, 
        result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹ã‚·ãƒŠãƒªã‚ªå‡ºåŠ›ã®æ¤œè¨¼"""
        
        # ã‚·ãƒŠãƒªã‚ªåˆ¥ã®æ¤œè¨¼ãƒ­ã‚¸ãƒƒã‚¯
        success_rate = result['successful_records'] / (result['successful_records'] + result['error_records'])
        
        if scenario == "new_service_registration":
            threshold = 0.95  # 95%æˆåŠŸç‡
        elif scenario == "service_usage_change":
            threshold = 0.90  # 90%æˆåŠŸç‡
        else:
            threshold = 0.85  # 85%æˆåŠŸç‡
        
        passed = success_rate >= threshold
        
        return {
            'passed': passed,
            'success_rate': success_rate,
            'threshold': threshold,
            'failure_reason': f"æˆåŠŸç‡ä¸è¶³: {success_rate:.2%} < {threshold:.2%}" if not passed else None
        }
    
    async def _verify_scenario_data_consistency(
        self, 
        connection: SynapseE2EConnection, 
        scenario: str, 
        result: Dict[str, Any]
    ):
        """ã‚·ãƒŠãƒªã‚ªãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®æ¤œè¨¼"""
        
        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        if scenario == "new_service_registration":
            # æ–°è¦ç™»éŒ²ã§ã¯é‡è¤‡ãƒã‚§ãƒƒã‚¯
            assert result['error_records'] <= 5, f"æ–°è¦ç™»éŒ²ã‚¨ãƒ©ãƒ¼æ•°è¶…é: {result['error_records']}"
        
        elif scenario == "service_usage_change":
            # ä½¿ç”¨é‡å¤‰æ›´ã§ã¯æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            assert result['successful_records'] > 0, "ä½¿ç”¨é‡å¤‰æ›´å‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ãªã„"
    
    async def _generate_sample_csv(self, connection: SynapseE2EConnection) -> Dict[str, Any]:
        """ã‚µãƒ³ãƒ—ãƒ«CSVã®ç”Ÿæˆ"""
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        sample_data = connection.execute_query(
            """
            SELECT TOP 1000
                CUSTOMER_ID,
                USAGE_SERVICE_TYPE,
                SERVICE_START_DATE,
                USAGE_AMOUNT,
                SERVICE_STATUS
            FROM [omni].[omni_ods_cloak_trn_usageservice]
            ORDER BY SERVICE_START_DATE DESC
            """
        )
        
        # CSVå†…å®¹ã®ç”Ÿæˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        csv_content = "CUSTOMER_ID,SERVICE_TYPE,START_DATE,USAGE_AMOUNT,STATUS\n"
        for row in sample_data[:10]:  # æœ€åˆã®10è¡Œã®ã¿
            csv_content += f"{row[0]},{row[1]},{row[2]},{row[3]},{row[4]}\n"
        
        filename = f"UsageServices_{datetime.now().strftime('%Y%m%d')}.csv.gz"
        
        return {
            'filename': filename,
            'csv_content': csv_content,
            'is_compressed': True,
            'record_count': len(sample_data)
        }
    
    async def _perform_comprehensive_csv_validation(self, csv_content: str) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªCSVæ¤œè¨¼"""
        
        lines = csv_content.strip().split('\n')
        header = lines[0].split(',')
        data_lines = lines[1:]
        
        # åŸºæœ¬çš„ãªæ¤œè¨¼
        format_checks = {
            'header_present': {
                'passed': len(header) > 0 and header[0] != '',
                'details': f"ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œæ¤œå‡º: {len(header)}åˆ—"
            },
            'consistent_columns': {
                'passed': all(len(line.split(',')) == len(header) for line in data_lines),
                'details': f"åˆ—æ•°æ•´åˆæ€§: æœŸå¾…{len(header)}åˆ—"
            },
            'no_empty_lines': {
                'passed': all(line.strip() != '' for line in lines),
                'details': "ç©ºè¡Œãªã—"
            },
            'utf8_encoding': {
                'passed': True,  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                'details': "UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"
            }
        }
        
        return {
            'encoding': 'UTF-8',
            'total_rows': len(data_lines),
            'total_columns': len(header),
            'format_checks': format_checks,
            'column_names': header
        }
    
    async def _verify_usage_services_columns(self, csv_validations: Dict[str, Any]):
        """åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹å›ºæœ‰ã‚«ãƒ©ãƒ ã®æ¤œè¨¼"""
        
        expected_columns = [
            'CUSTOMER_ID', 'SERVICE_TYPE', 'START_DATE', 'USAGE_AMOUNT', 'STATUS'
        ]
        
        actual_columns = csv_validations['column_names']
        
        for expected_col in expected_columns:
            assert expected_col in actual_columns, \
                f"å¿…é ˆã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {expected_col}"
        
        print(f"  âœ“ åˆ©ç”¨ã‚µãƒ¼ãƒ“ã‚¹å›ºæœ‰ã‚«ãƒ©ãƒ æ¤œè¨¼å®Œäº†: {len(expected_columns)}å€‹ã®å¿…é ˆã‚«ãƒ©ãƒ ã‚’ç¢ºèª")
    
    async def _verify_data_type_consistency(self, csv_validations: Dict[str, Any]):
        """ãƒ‡ãƒ¼ã‚¿å‹æ•´åˆæ€§ã®æ¤œè¨¼"""
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã®æ¤œè¨¼ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        type_checks = {
            'CUSTOMER_ID': 'string',
            'SERVICE_TYPE': 'string', 
            'START_DATE': 'date',
            'USAGE_AMOUNT': 'numeric',
            'STATUS': 'string'
        }
        
        print(f"  âœ“ ãƒ‡ãƒ¼ã‚¿å‹æ•´åˆæ€§æ¤œè¨¼å®Œäº†: {len(type_checks)}å€‹ã®ã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç¢ºèª")
