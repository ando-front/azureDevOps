#!/usr/bin/env python3
"""
E2Eç’°å¢ƒã®åŒ…æ‹¬çš„æ•´å‚™ã¨ãƒ†ã‚¹ãƒˆæˆåŠŸç‡å‘ä¸Šã®ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
600ä»¥ä¸Šã®ãƒ†ã‚¹ãƒˆã‚’æˆåŠŸã•ã›ã‚‹ãŸã‚ã®çµ±ä¸€çš„ãªç’°å¢ƒæ§‹ç¯‰
"""

import pyodbc
import logging
from datetime import datetime
from typing import Dict, List, Optional
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ComprehensiveE2EEnvironmentBuilder:
    def __init__(self):
        self.conn_str = (
            "DRIVER={ODBC Driver 18 for SQL Server};"
            "SERVER=localhost,1433;"
            "DATABASE=TGMATestDB;"
            "UID=sa;"
            "PWD=YourStrong!Passw0rd123;"
            "TrustServerCertificate=yes;"
            "Encrypt=no;"  # E2Eãƒ†ã‚¹ãƒˆã§ã¯æš—å·åŒ–ç„¡åŠ¹
        )
        
    def get_connection(self):
        """SQL Serveræ¥ç¶šã‚’å–å¾—"""
        return pyodbc.connect(self.conn_str)
    
    def create_missing_tables(self):
        """ä¸è¶³ã—ã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ"""
        logger.info("ğŸ—ï¸ ä¸è¶³ã—ã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆä¸­...")
        
        table_definitions = {
            "raw_data_source": """
                CREATE TABLE [dbo].[raw_data_source] (
                    [source_id] INT IDENTITY(1,1) PRIMARY KEY,
                    [source_name] NVARCHAR(100) NOT NULL,
                    [source_type] NVARCHAR(50) NOT NULL,
                    [data_format] NVARCHAR(50),
                    [connection_string] NVARCHAR(500),
                    [last_modified] DATETIME2 DEFAULT GETDATE(),
                    [is_active] BIT DEFAULT 1,
                    [created_at] DATETIME2 DEFAULT GETDATE()
                );
            """,
            
            "watermark_table": """
                CREATE TABLE [dbo].[watermark_table] (
                    [watermark_id] INT IDENTITY(1,1) PRIMARY KEY,
                    [table_name] NVARCHAR(100) NOT NULL,
                    [last_processed_timestamp] DATETIME2,
                    [last_processed_id] BIGINT,
                    [batch_id] NVARCHAR(100),
                    [created_at] DATETIME2 DEFAULT GETDATE(),
                    [updated_at] DATETIME2 DEFAULT GETDATE()
                );
            """,
            
            "data_quality_metrics": """
                CREATE TABLE [dbo].[data_quality_metrics] (
                    [metric_id] INT IDENTITY(1,1) PRIMARY KEY,
                    [source_table] NVARCHAR(100) NOT NULL,
                    [metric_name] NVARCHAR(100) NOT NULL,
                    [metric_value] DECIMAL(10,4),
                    [threshold_value] DECIMAL(10,4),
                    [status] NVARCHAR(20),
                    [measured_at] DATETIME2 DEFAULT GETDATE(),
                    [pipeline_execution_id] NVARCHAR(100)
                );
            """,
            
            "etl_error_log": """
                CREATE TABLE [dbo].[etl_error_log] (
                    [error_id] INT IDENTITY(1,1) PRIMARY KEY,
                    [pipeline_name] NVARCHAR(100),
                    [error_type] NVARCHAR(50),
                    [error_message] NVARCHAR(MAX),
                    [error_severity] NVARCHAR(20),
                    [error_timestamp] DATETIME2 DEFAULT GETDATE(),
                    [source_table] NVARCHAR(100),
                    [affected_records] INT DEFAULT 0,
                    [recovery_action] NVARCHAR(500)
                );
            """,
            
            "pipeline_performance_metrics": """
                CREATE TABLE [dbo].[pipeline_performance_metrics] (
                    [metric_id] INT IDENTITY(1,1) PRIMARY KEY,
                    [pipeline_name] NVARCHAR(100) NOT NULL,
                    [execution_id] NVARCHAR(100),
                    [start_time] DATETIME2,
                    [end_time] DATETIME2,
                    [duration_seconds] INT,
                    [records_processed] BIGINT DEFAULT 0,
                    [throughput_records_per_sec] DECIMAL(10,2),
                    [cpu_usage_percent] DECIMAL(5,2),
                    [memory_usage_mb] DECIMAL(10,2),
                    [status] NVARCHAR(20),
                    [created_at] DATETIME2 DEFAULT GETDATE()
                );
            """
        }
        
        conn = self.get_connection()
        cursor = conn.cursor()
        
        for table_name, definition in table_definitions.items():
            try:
                # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                cursor.execute(f"""
                    SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES 
                    WHERE TABLE_NAME = '{table_name}' AND TABLE_SCHEMA = 'dbo'
                """)
                
                if cursor.fetchone()[0] == 0:
                    logger.info(f"ğŸ“‹ Creating table: {table_name}")
                    cursor.execute(definition)
                    conn.commit()
                else:
                    logger.info(f"âœ… Table {table_name} already exists")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Error creating table {table_name}: {str(e)}")
        
        conn.close()
    
    def insert_required_test_data(self):
        """å¿…è¦ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥"""
        logger.info("ğŸ“Š å¿…è¦ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥ä¸­...")
        
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            # raw_data_sourceãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥ï¼ˆIDENTITY_INSERTã‚’ä½¿ç”¨ï¼‰
            cursor.execute("SET IDENTITY_INSERT [dbo].[raw_data_source] ON")
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢
            cursor.execute("DELETE FROM [dbo].[raw_data_source]")
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
            test_sources = [
                (1, 'customer', 'database', 'SQL', 'server=localhost;database=source', datetime.now(), 1),
                (2, 'order', 'database', 'SQL', 'server=localhost;database=source', datetime.now(), 1),
                (3, 'product', 'database', 'SQL', 'server=localhost;database=source', datetime.now(), 1)
            ]
            
            for source in test_sources:
                cursor.execute("""
                    INSERT INTO [dbo].[raw_data_source] 
                    (source_id, source_name, source_type, data_format, connection_string, last_modified, is_active)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, source)
            
            cursor.execute("SET IDENTITY_INSERT [dbo].[raw_data_source] OFF")
            
            # watermark_tableã«ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
            cursor.execute("DELETE FROM [dbo].[watermark_table]")
            watermarks = [
                ('client_dm', datetime.now(), 1000, 'BATCH_001'),
                ('ClientDmBx', datetime.now(), 2000, 'BATCH_002'),
                ('marketing_client_dm', datetime.now(), 3000, 'BATCH_003')
            ]
            
            for watermark in watermarks:
                cursor.execute("""
                    INSERT INTO [dbo].[watermark_table] 
                    (table_name, last_processed_timestamp, last_processed_id, batch_id)
                    VALUES (?, ?, ?, ?)
                """, watermark)
            
            # pipeline_performance_metricsã«ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
            cursor.execute("DELETE FROM [dbo].[pipeline_performance_metrics]")
            cursor.execute("""
                INSERT INTO [dbo].[pipeline_performance_metrics]
                (pipeline_name, execution_id, start_time, end_time, duration_seconds, 
                 records_processed, throughput_records_per_sec, cpu_usage_percent, 
                 memory_usage_mb, status)
                VALUES 
                ('test_pipeline', 'exec_001', GETDATE(), DATEADD(SECOND, 30, GETDATE()), 
                 30, 1000, 33.33, 45.5, 256.7, 'SUCCESS')
            """)
            
            conn.commit()
            logger.info("âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æŒ¿å…¥å®Œäº†")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {str(e)}")
            conn.rollback()
        finally:
            conn.close()
    
    def fix_encryption_compatibility(self):
        """æš—å·åŒ–è¨­å®šã®äº’æ›æ€§ã‚’ä¿®æ­£"""
        logger.info("ğŸ” æš—å·åŒ–è¨­å®šã®äº’æ›æ€§ã‚’ä¿®æ­£ä¸­...")
        
        # ã™ã¹ã¦ã®E2Eãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã§æš—å·åŒ–è¨­å®šã‚’çµ±ä¸€
        helper_files = [
            "/Users/andokenji/Documents/æ›¸é¡ - å®‰è—¤è³¢äºŒã®Mac mini/GitHub/azureDevOps/tests/e2e/helpers/synapse_e2e_helper.py",
            "/Users/andokenji/Documents/æ›¸é¡ - å®‰è—¤è³¢äºŒã®Mac mini/GitHub/azureDevOps/tests/helpers/reproducible_e2e_helper.py"
        ]
        
        for file_path in helper_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Encrypt=yesã‚’Encrypt=noã«å¤‰æ›´
                updated_content = content.replace('Encrypt=yes', 'Encrypt=no')
                
                if updated_content != content:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(updated_content)
                    logger.info(f"âœ… æš—å·åŒ–è¨­å®šã‚’ä¿®æ­£: {file_path}")
                else:
                    logger.info(f"ğŸ‘ æš—å·åŒ–è¨­å®šã¯æ—¢ã«æ­£ã—ã„: {file_path}")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£ã‚¨ãƒ©ãƒ¼ {file_path}: {str(e)}")
    
    def validate_environment(self):
        """ç’°å¢ƒãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ã‚’æ¤œè¨¼"""
        logger.info("ğŸ” ç’°å¢ƒæ¤œè¨¼ä¸­...")
        
        conn = self.get_connection()
        cursor = conn.cursor()
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª
        required_tables = [
            'client_dm', 'ClientDmBx', 'marketing_client_dm', 'point_grant_email',
            'pipeline_execution_log', 'raw_data_source', 'watermark_table',
            'data_quality_metrics', 'etl_error_log', 'pipeline_performance_metrics'
        ]
        
        missing_tables = []
        for table in required_tables:
            cursor.execute(f"""
                SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES 
                WHERE TABLE_NAME = '{table}' AND TABLE_SCHEMA = 'dbo'
            """)
            if cursor.fetchone()[0] == 0:
                missing_tables.append(table)
        
        if missing_tables:
            logger.warning(f"âš ï¸ ä¸è¶³ã—ã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«: {missing_tables}")
        else:
            logger.info("âœ… ã™ã¹ã¦ã®å¿…è¦ãªãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ã¾ã™")
        
        # ãƒ‡ãƒ¼ã‚¿å­˜åœ¨ç¢ºèª
        for table in ['client_dm', 'raw_data_source', 'watermark_table']:
            try:
                cursor.execute(f"SELECT COUNT(*) FROM [dbo].[{table}]")
                count = cursor.fetchone()[0]
                logger.info(f"ğŸ“Š {table}: {count} ãƒ¬ã‚³ãƒ¼ãƒ‰")
            except Exception as e:
                logger.warning(f"âš ï¸ {table}ã®ãƒ‡ãƒ¼ã‚¿ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        conn.close()
    
    def run_comprehensive_setup(self):
        """åŒ…æ‹¬çš„ãªE2Eç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
        logger.info("ğŸš€ åŒ…æ‹¬çš„E2Eç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹...")
        
        try:
            self.create_missing_tables()
            self.insert_required_test_data()
            self.fix_encryption_compatibility()
            self.validate_environment()
            
            logger.info("ğŸ‰ åŒ…æ‹¬çš„E2Eç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

if __name__ == "__main__":
    builder = ComprehensiveE2EEnvironmentBuilder()
    success = builder.run_comprehensive_setup()
    
    if success:
        print("âœ… E2Eç’°å¢ƒã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚600ä»¥ä¸Šã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹æº–å‚™ãŒã§ãã¦ã„ã¾ã™ã€‚")
    else:
        print("âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
